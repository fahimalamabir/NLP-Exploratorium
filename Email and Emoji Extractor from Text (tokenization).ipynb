{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to tokenization\n",
    "Tokenization is the process of transforming a string or document into smaller chunks, which we call tokens. This is usually one step in the process of preparing a text for natural language processing. \n",
    "\n",
    "#### Understanding Tokenization\n",
    " What are Tokens?\n",
    "\n",
    "- Tokens are the building blocks of natural language processing. They are essentially the pieces that comprise a piece of text, similar to words in a sentence or sentences in a paragraph.\n",
    "\n",
    "Types of Tokenization:\n",
    "\n",
    "- Word Tokenization: This involves breaking text into individual words. For example, the sentence \"Hello world!\" would be tokenized into two tokens: \"Hello\" and \"world!\".\n",
    "- Sentence Tokenization: This involves breaking text into individual sentences. It’s useful for processing tasks that require understanding the context of each sentence.\n",
    "\n",
    "Why Tokenization?\n",
    "\n",
    "- Tokenization is used to structure text for further analysis or processing, like part-of-speech tagging, sentiment analysis, or input for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'There', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "word_tokenize(\"Hi There!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "file_path = 'scene_one.txt'\n",
    "\n",
    "# Reading the file\n",
    "with open(file_path, 'r') as file:\n",
    "    scene_one = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: \\n[wind] [clop clop clop] \\\\nKING ARTHUR: Whoa there!',\n",
       " '[clop clop clop] \\\\nSOLDIER #1: Halt!',\n",
       " 'Who goes there?\\\\nARTHUR: It is I, Arthur, son of \\n...\\ncreeper!\\\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\\\nSOLDIER #2: Well, why not?\\\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who',\n",
       " 'goes',\n",
       " 'there',\n",
       " '?',\n",
       " '\\\\nARTHUR',\n",
       " ':',\n",
       " 'It',\n",
       " 'is',\n",
       " 'I',\n",
       " ',',\n",
       " 'Arthur',\n",
       " ',',\n",
       " 'son',\n",
       " 'of',\n",
       " '...',\n",
       " 'creeper',\n",
       " '!',\n",
       " '\\\\nSOLDIER',\n",
       " '#',\n",
       " '1',\n",
       " ':',\n",
       " 'What',\n",
       " ',',\n",
       " 'held',\n",
       " 'under',\n",
       " 'the',\n",
       " 'dorsal',\n",
       " 'guiding',\n",
       " 'feathers',\n",
       " '?',\n",
       " '\\\\nSOLDIER',\n",
       " '#',\n",
       " '2',\n",
       " ':',\n",
       " 'Well',\n",
       " ',',\n",
       " 'why',\n",
       " 'not',\n",
       " '?',\n",
       " '\\\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use word_tokenize to tokenize the 3rd sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[2])\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guiding', 'Well', ',', 'there', 'goes', 'Arthur', 'of', 'the', 'ARTHUR', '\\\\nARTHUR', 'Who', 'wind', 'is', 'clop', 'feathers', '#', 'SCENE', '\\\\n', 'I', 'under', 'It', 'not', '...', '1', ']', '!', '2', '\\\\nSOLDIER', 'What', 'Whoa', 'creeper', ':', 'son', 'held', 'why', 'dorsal', 'Halt', '[', '\\\\nKING', '?'}\n"
     ]
    }
   ],
   "source": [
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    " \n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use re.search() to search for the first occurrence of the word \"wind\" in scene_one. Store the result in match.\n",
    "Print the start and end indexes of match using its .start() and .end() methods, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Search for the first occurrence of \"wind\" in scene_one: match\n",
    "match = re.search(\"wind\", scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 15\n"
     ]
    }
   ],
   "source": [
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a regular expression called pattern1 to find anything in square brackets.\n",
    "Use re.search() with the pattern to find the first text in scene_one in square brackets in the scene. Print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(10, 78), match='[wind] [clop clop clop] \\\\nKING ARTHUR: Whoa ther>\n"
     ]
    }
   ],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pattern to match the script notation (e.g. Character:), assigning the result to pattern2. Remember that you will want to match any words or spaces that precede the : (such as the space within SOLDIER #1:).\n",
    "Use re.match() with your new pattern to find and print the script notation in the fourth line. The tokenized sentences are available in your namespace as sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 8), match='SCENE 1:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the 1st sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2,sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced tokenization with NLTK and regex\n",
    "\n",
    "One new regex pattern you will find useful for advanced tokenization is the ability to use the or method. In regex, OR is represented by the pipe character |. To use the or, you can define a group using parenthesis (). Groups can be either a pattern or a set of characters you want to match. You can also define explicit character classes using square brackets []. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE',\n",
       " '1',\n",
       " 'wind',\n",
       " 'clop',\n",
       " 'clop',\n",
       " 'clop',\n",
       " 'nKING',\n",
       " 'ARTHUR',\n",
       " 'Whoa',\n",
       " 'there',\n",
       " 'clop',\n",
       " 'clop',\n",
       " 'clop',\n",
       " 'nSOLDIER',\n",
       " '1',\n",
       " 'Halt',\n",
       " 'Who',\n",
       " 'goes',\n",
       " 'there',\n",
       " 'nARTHUR',\n",
       " 'It',\n",
       " 'is',\n",
       " 'I',\n",
       " 'Arthur',\n",
       " 'son',\n",
       " 'of',\n",
       " 'creeper',\n",
       " 'nSOLDIER',\n",
       " '1',\n",
       " 'What',\n",
       " 'held',\n",
       " 'under',\n",
       " 'the',\n",
       " 'dorsal',\n",
       " 'guiding',\n",
       " 'feathers',\n",
       " 'nSOLDIER',\n",
       " '2',\n",
       " 'Well',\n",
       " 'why',\n",
       " 'not',\n",
       " 'n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_letter = ('(\\d+|\\w+)')\n",
    "re.findall(digit_letter,scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE',\n",
       " '[wind]',\n",
       " '[clop',\n",
       " 'clop',\n",
       " 'clop]',\n",
       " '\\\\nKING',\n",
       " 'ARTHUR',\n",
       " 'Whoa',\n",
       " 'there',\n",
       " '[clop',\n",
       " 'clop',\n",
       " 'clop]',\n",
       " '\\\\nSOLDIER',\n",
       " 'Halt',\n",
       " 'Who',\n",
       " 'goes',\n",
       " 'there',\n",
       " '\\\\nARTHUR',\n",
       " 'It',\n",
       " 'is',\n",
       " 'I',\n",
       " 'Arthur',\n",
       " 'son',\n",
       " 'of',\n",
       " 'creeper',\n",
       " '\\\\nSOLDIER',\n",
       " 'What',\n",
       " 'held',\n",
       " 'under',\n",
       " 'the',\n",
       " 'dorsal',\n",
       " 'guiding',\n",
       " 'feathers',\n",
       " '\\\\nSOLDIER',\n",
       " 'Well',\n",
       " 'why',\n",
       " 'not',\n",
       " '\\\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letter = r'[a-zA-z]+'\n",
    "re.findall(all_letter,scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(6, 7), match='1'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = r'[0-9]'\n",
    "re.search(num,scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '#', '1', '#', '1', '#', '2']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special = r'[\\d+|#]' \n",
    "re.findall(special, scene_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a tokenizer\n",
    "\n",
    "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "\n",
    "You can use regexp_tokenize(string, pattern) with my_string and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = '(\\\\w+|\\\\?|!)'\n",
    "pattern2 = '(\\\\w+|#\\\\d|\\\\?|!)'\n",
    "pattern3 = '(#\\\\d\\\\w+\\\\?!)'\n",
    "pattern4 = '\\\\s+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(my_string, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(my_string, pattern4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex with NLTK tokenization\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n",
    "\n",
    "Unlike the syntax for the regex library, with nltk_tokenize() you pass the pattern as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n",
    "# Call regexp_tokenize() with this hashtag pattern on the first tweet in tweets and assign the result to hashtags.\n",
    "# Print hashtags\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a new pattern called pattern2 to match mentions and hashtags. A mention is something like @DataCamp.\n",
    "\n",
    "Then, call regexp_tokenize() with your new hashtag pattern on the last tweet in tweets and assign the result to mentions_hashtags.\n",
    "\n",
    "You can access the last element of a list using -1 as the index, for example, tweets[-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of TweetTokenizer called tknzr and use it inside a list comprehension to tokenize each tweet into a new list called all_tokens.\n",
    "To do this, use the .tokenize() method of tknzr, with t as your iterator variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.tokenize.casual.TweetTokenizer at 0x1c8c5caef10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "tknzr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-ascii tokenization\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = \"Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Über']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email and Emoji Extractor from Text\n",
    "\n",
    "### Objective\n",
    "Develop a Python script that can take a large string (such as the content of a document) and extract all the email addresses and emojis it contains.\n",
    "\n",
    "Understanding Regex for Emails and Emojis:\n",
    "\n",
    "\n",
    "The regular expression r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' is designed to match email addresses in a text. Let's break it down to understand each part:\n",
    "\n",
    "- \\b: This is a word boundary. It ensures that the pattern matches only a complete word, not a substring of a word. For example, it prevents matching 'abc@def' in 'xyzabc@defghij'.\n",
    "\n",
    "- [A-Za-z0-9._%+-]+: This part matches the username of an email address. It can include: Uppercase and lowercase letters (A-Za-z). Digits (0-9). Special characters like dot (.), underscore (_), percent (%), plus (+), and hyphen (-).\n",
    "\n",
    "- The + means that the preceding character set can appear one or more times. @: This is the literal \"at\" symbol that appears in all email addresses. [A-Za-z0-9.-]+: This part matches the domain name of the email address. It can include: Uppercase and lowercase letters. Digits. Dots and hyphens. The + here again means one or more occurrences of the preceding character set.\n",
    "\\.\n",
    "\n",
    "- This is a literal dot. In regex, a dot is a special character that matches almost any character, so it's escaped with a backslash to denote an actual dot character. [A-Z|a-z]{2,}: This matches the top-level domain (like .com, .org, .net). It includes: Uppercase and lowercase letters. The {2,} specifies that this part must be at least two characters long, with no upper limit. The pipe | inside the brackets is not doing its usual function of \"or\" in this context; it's being interpreted literally, which is actually a mistake in the pattern. It should be removed to correctly enforce the rule. \\b: Another word boundary, to ensure that the pattern matches only a complete word.\n",
    "\n",
    "For emojis, the regex can be more complex. A basic pattern to match common emojis is something like r'[\\U0001F600-\\U0001F64F]'. This pattern matches a range of Unicode characters that include emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regular expressions for emails and emojis\n",
    "email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "emoji_pattern = re.compile(\n",
    "    r'[\\U0001F600-\\U0001F64F'  # emoticons\n",
    "    r'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "    r'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "    r'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "    r'\\U00002702-\\U000027B0'\n",
    "    r'\\U000024C2-\\U0001F251'\n",
    "    r'\\U0001f926-\\U0001f937'\n",
    "    r'\\U00010000-\\U0010ffff'\n",
    "    r'\\u2640-\\u2642' \n",
    "    r'\\u2600-\\u2B55'\n",
    "    r'\\u200d'\n",
    "    r'\\u23cf'\n",
    "    r'\\u23e9'\n",
    "    r'\\u231a'\n",
    "    r'\\ufe0f'  # dingbats\n",
    "    r'\\u3030'\n",
    "    ']+', flags=re.UNICODE)\n",
    "\n",
    "# Function to extract emails\n",
    "def extract_emails(text):\n",
    "    return re.findall(email_pattern, text)\n",
    "\n",
    "# Function to extract emojis\n",
    "def extract_emojis(text):\n",
    "    return re.findall(emoji_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "file_path1 = 'fake2.txt'\n",
    "\n",
    "# Reading the file\n",
    "with open(file_path1, 'r',encoding='utf-8') as file:\n",
    "    example_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting emails and emojis\n",
    "emails = extract_emails(example_text)\n",
    "emojis = extract_emojis(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contact@fakemail.com', 'hr@learningcorp.net', 'helpdesk@techsolutions.org', 'orders@online-shop.com', 'community@socialplatform.io', 'unsubscribe@newsmail.com', 'partners@b2bmarket.net', 'security@accountsafety.com', 'feedback@userinput.com', 'service@homefixers.org', 'press@mediarelations.com', 'sales@techproducts.net', 'legal@lawconsultants.net', 'techsupport@troubleshooters.org', 'travel@adventures.com']\n"
     ]
    }
   ],
   "source": [
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😊', '📅', '📞', '🛒', '🌐', '🚫', '🤝', '🔒', '📝', '🛠️', '🎤', '💻', '⚖️', '💡', '✈️']\n"
     ]
    }
   ],
   "source": [
    "print(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
